{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "nlp-1 (Python3)",
      "language": "python",
      "name": "nlp-1"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "scrape.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yh0CiJnRsBXI",
        "colab_type": "code",
        "outputId": "52e8cd52-32ee-4606-bc48-d828371941c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "!pip install squarify"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting squarify\n",
            "  Downloading https://files.pythonhosted.org/packages/0b/2b/2e77c35326efec19819cd1d729540d4d235e6c2a3f37658288a363a67da5/squarify-0.4.3-py3-none-any.whl\n",
            "Installing collected packages: squarify\n",
            "Successfully installed squarify-0.4.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y133qOkzrDND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "import squarify\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "from spacy.tokenizer import Tokenizer\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import html as ihtml\n",
        "\n",
        "import requests\n",
        "import sqlite3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCvFe_74rDNa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_job_title(soup):\n",
        "    jobs = []\n",
        "    for div in soup.find_all(name=\"div\", attrs={\"class\":\"row\"}):\n",
        "        for a in div.find_all(name=\"a\", attrs={\"data-tn-element\":\"jobTitle\"}):\n",
        "            jobs.append(a[\"title\"])\n",
        "    return(jobs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfMKEEs5rDNl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_company(soup):\n",
        "    companies = []\n",
        "    for div in soup.find_all(name=\"div\", attrs={\"class\":\"row\"}):\n",
        "        company = div.find_all(name=\"span\", attrs={\"class\":\"company\"})\n",
        "        if len(company) > 0:\n",
        "            for b in company:\n",
        "                companies.append(b.text.strip())\n",
        "        else:\n",
        "            sec_try = div.find_all(name=\"span\", attrs={\"class\":\"result-linl-source\"})\n",
        "            for span in sec_try:\n",
        "                companies.append(span.text.strip())\n",
        "    return(companies)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onup3OzurDNt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_location(soup):\n",
        "    locations = []\n",
        "    for div in soup.find_all(name=\"div\", attrs={\"class\":\"row\"}):\n",
        "        try:\n",
        "            locations.append(div.find(name=\"span\", attrs={\"class\":\"location\"}).text)\n",
        "        except:\n",
        "            locations.append(\"None\")\n",
        "    return(locations)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwxQyC4xrDN0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_salary(soup):\n",
        "    salaries = []\n",
        "    for div in soup.find_all(name=\"div\", attrs={\"class\":\"row\"}):\n",
        "        try:\n",
        "            salaries.append(div.find(name=\"span\", attrs={\"class\":\"salaryText\"}).text.replace(\"\\n\",\"\"))\n",
        "        except:\n",
        "            salaries.append(\"None\")\n",
        "    return(salaries)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lG1FO5rgrDN7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_job_title(soup):\n",
        "    jobs = []\n",
        "    for div in soup.find_all(name=\"div\", attrs={\"class\":\"row\"}):\n",
        "        for a in div.find_all(name=\"a\", attrs={\"data-tn-element\":\"jobTitle\"}):\n",
        "            jobs.append(a[\"title\"])\n",
        "    return(jobs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z49zqaYvrDOB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_url(soup):\n",
        "    urls = []\n",
        "    for div1 in soup.find_all(name=\"div\", attrs={\"class\":\"row\"}):\n",
        "        for div2 in div1.find_all(name=\"div\", attrs={\"class\":\"title\"}):\n",
        "            for a in div2.find_all(name=\"a\", href=True):\n",
        "                urls.append(a['href'])\n",
        "    return(urls)\n",
        "\n",
        "def extract_desc(urls):\n",
        "    descs = []\n",
        "    for url in urls:\n",
        "        full_url = \"https://www.indeed.com\" + url\n",
        "        detail_page = requests.get(full_url)\n",
        "        detail_soup = BeautifulSoup(detail_page.text, \"html.parser\")\n",
        "        \n",
        "        for div in detail_soup.find_all(name=\"div\", attrs={\"id\":\"jobDescriptionText\"}):\n",
        "            descs.append(div.text)\n",
        "    return(descs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuIAkmwirDOG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_count(soup):\n",
        "    for div in soup.find_all(name=\"div\", attrs={\"id\":\"searchCountPages\"}):\n",
        "        temp_str = div.text.replace(\" \", \"\")\n",
        "        temp_count_str = re.search('of(.*)jobs', temp_str)\n",
        "        count_str = re.sub('[^0-9]','', temp_count_str.group(1))\n",
        "        count = int(count_str)\n",
        "    return(count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apMUcrzVrDOM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_list(title_name, city_name, st_name):\n",
        "    max_results = 100\n",
        "    columns = [\"city\", \"job_title\", \"company\", \"location\", \"salary\", \"description\"]\n",
        "\n",
        "    city_url = \"https://www.indeed.com/jobs?q=\" + title_name + \\\n",
        "               \"&l=\" + city_name + \"%2C+\" + st_name\n",
        "    page = requests.get(city_url)\n",
        "    soup = BeautifulSoup(page.text, \"html.parser\")\n",
        "    max_counter = extract_count(soup)\n",
        "    print(max_counter, title_name, city_name)\n",
        "   \n",
        "    job_title_list = []\n",
        "    company_list = []\n",
        "    location_list = []\n",
        "    salary_list = []\n",
        "    desc_list = []\n",
        "\n",
        "    for start in range(0, max_results, 10):\n",
        "        city_url = \"https://www.indeed.com/jobs?q=\" + title_name + \\\n",
        "                   \"&l=\" + city_name + \"%2C+\" + st_name + \\\n",
        "                   \"&start=\" + str(start)\n",
        "        page = requests.get(city_url)\n",
        "        soup = BeautifulSoup(page.text, \"html.parser\")\n",
        "\n",
        "        job_title_list.extend(extract_job_title(soup))\n",
        "        company_list.extend(extract_company(soup))\n",
        "        location_list.extend(extract_location(soup))\n",
        "        salary_list.extend(extract_salary(soup))\n",
        "        add_urls = extract_url(soup)\n",
        "        desc_list.extend(extract_desc(add_urls))\n",
        "\n",
        "    return job_title_list, company_list, location_list, salary_list, desc_list, max_counter\n",
        "\n",
        "def job_db(title, city, st):   \n",
        "    j_list, c_list, l_list, s_list, d_list, m_counter = extract_list(title, city, st)     \n",
        "    temp_df = pd.DataFrame(list(zip(j_list, c_list, l_list, s_list, d_list)), \n",
        "                          columns = ['job_title', 'company', 'location', 'salary', 'description'])\n",
        "    temp_df['counts'] = m_counter\n",
        "    city_name = city.replace('+', ' ')\n",
        "    temp_df['city'] = city_name\n",
        "    return temp_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btT5V6pSNCsj",
        "colab_type": "code",
        "outputId": "17d169a9-a344-4593-dd9a-82e2eae3e293",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "temp1_df = job_db('data+scientist', 'San+Jose', 'CA')\n",
        "temp2_df = job_db('data+scientist', 'San+Francisco', 'CA')\n",
        "temp3_df = job_db('data+scientist', 'Seattle', 'WA')\n",
        "temp4_df = job_db('data+scientist', 'Washington', 'DC')\n",
        "temp5_df = job_db('data+scientist', 'New+York', 'NY')\n",
        "temp6_df = job_db('data+scientist', 'Baltimore', 'MD')\n",
        "temp7_df = job_db('data+scientist', 'Boulder', 'CO')\n",
        "temp8_df = job_db('data+scientist', 'San+Diego', 'CA')\n",
        "temp9_df = job_db('data+scientist', 'Denver', 'CO')\n",
        "temp10_df = job_db('data+scientist', 'Huntsville', 'AL')\n",
        "temp11_df = job_db('data+scientist', 'Colorado+Springs', 'CO')\n",
        "temp12_df = job_db('data+scientist', 'Houston', 'TX')\n",
        "temp13_df = job_db('data+scientist', 'Trenton', 'NJ')\n",
        "temp14_df = job_db('data+scientist', 'Dallas', 'TX')\n",
        "temp15_df = job_db('data+scientist', 'Columbus', 'OH')\n",
        "temp16_df = job_db('data+scientist', 'Austin', 'TX')\n",
        "temp17_df = job_db('data+scientist', 'Philadelphia', 'PA')\n",
        "temp18_df = job_db('data+scientist', 'Durham', 'NC')\n",
        "temp19_df = job_db('data+scientist', 'Raleigh', 'NC')\n",
        "temp20_df = job_db('data+scientist', 'Atlanta', 'GA')\n",
        "\n",
        "data_scientist_df = pd.concat([temp1_df, temp2_df, temp3_df, temp4_df, temp5_df,\n",
        "                       temp6_df, temp7_df, temp8_df, temp9_df, temp10_df,\n",
        "                       temp11_df, temp12_df, temp13_df, temp14_df, temp15_df,\n",
        "                       temp16_df, temp17_df, temp18_df, temp19_df, temp20_df], ignore_index=True)\n",
        "\n",
        "print(data_scientist_df.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1326 data+scientist San+Jose\n",
            "937 data+scientist San+Francisco\n",
            "1109 data+scientist Seattle\n",
            "1170 data+scientist Washington\n",
            "985 data+scientist New+York\n",
            "164 data+scientist Baltimore\n",
            "160 data+scientist Boulder\n",
            "164 data+scientist San+Diego\n",
            "179 data+scientist Denver\n",
            "21 data+scientist Huntsville\n",
            "6 data+scientist Colorado+Springs\n",
            "99 data+scientist Houston\n",
            "91 data+scientist Trenton\n",
            "257 data+scientist Dallas\n",
            "74 data+scientist Columbus\n",
            "185 data+scientist Austin\n",
            "216 data+scientist Philadelphia\n",
            "145 data+scientist Durham\n",
            "147 data+scientist Raleigh\n",
            "259 data+scientist Atlanta\n",
            "(1943, 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtJLD1unfnnG",
        "colab_type": "code",
        "outputId": "8cda3805-e1e2-4df3-c2ac-f5be6f37d799",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "temp1_df = job_db('web+developer', 'San+Jose', 'CA')\n",
        "temp2_df = job_db('web+developer', 'San+Francisco', 'CA')\n",
        "temp3_df = job_db('web+developer', 'Seattle', 'WA')\n",
        "temp4_df = job_db('web+developer', 'Washington', 'DC')\n",
        "temp5_df = job_db('web+developer', 'New+York', 'NY')\n",
        "temp6_df = job_db('web+developer', 'Baltimore', 'MD')\n",
        "temp7_df = job_db('web+developer', 'Boulder', 'CO')\n",
        "temp8_df = job_db('web+developer', 'San+Diego', 'CA')\n",
        "temp9_df = job_db('web+developer', 'Denver', 'CO')\n",
        "temp10_df = job_db('web+developer', 'Huntsville', 'AL')\n",
        "temp11_df = job_db('web+developer', 'Colorado+Springs', 'CO')\n",
        "temp12_df = job_db('web+developer', 'Houston', 'TX')\n",
        "temp13_df = job_db('web+developer', 'Trenton', 'NJ')\n",
        "temp14_df = job_db('web+developer', 'Dallas', 'TX')\n",
        "temp15_df = job_db('web+developer', 'Columbus', 'OH')\n",
        "temp16_df = job_db('web+developer', 'Austin', 'TX')\n",
        "temp17_df = job_db('web+developer', 'Philadelphia', 'PA')\n",
        "temp18_df = job_db('web+developer', 'Durham', 'NC')\n",
        "temp19_df = job_db('web+developer', 'Raleigh', 'NC')\n",
        "\n",
        "web_developer_df = pd.concat([temp1_df, temp2_df, temp3_df, temp4_df, temp5_df,\n",
        "                              temp6_df, temp7_df, temp8_df, temp9_df, temp10_df,\n",
        "                              temp11_df, temp12_df, temp13_df, temp14_df, temp15_df,\n",
        "                              temp16_df, temp17_df, temp18_df, temp19_df, temp20_df], ignore_index=True)\n",
        "print(web_developer_df.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1414 web+developer San+Jose\n",
            "1417 web+developer San+Francisco\n",
            "1419 web+developer Seattle\n",
            "3387 web+developer Washington\n",
            "1389 web+developer New+York\n",
            "876 web+developer Baltimore\n",
            "440 web+developer Boulder\n",
            "387 web+developer San+Diego\n",
            "598 web+developer Denver\n",
            "78 web+developer Huntsville\n",
            "68 web+developer Colorado+Springs\n",
            "368 web+developer Houston\n",
            "224 web+developer Trenton\n",
            "777 web+developer Dallas\n",
            "224 web+developer Columbus\n",
            "653 web+developer Austin\n",
            "532 web+developer Philadelphia\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxjzVabzmpRJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp1_df = job_db('ux+designer', 'San+Jose', 'CA')\n",
        "temp2_df = job_db('ux+designer', 'San+Francisco', 'CA')\n",
        "temp3_df = job_db('ux+designer', 'Seattle', 'WA')\n",
        "temp4_df = job_db('ux+designer', 'Washington', 'DC')\n",
        "temp5_df = job_db('ux+designer', 'New+York', 'NY')\n",
        "temp6_df = job_db('ux+designer', 'Baltimore', 'MD')\n",
        "temp7_df = job_db('ux+designer', 'Boulder', 'CO')\n",
        "temp8_df = job_db('ux+designer', 'San+Diego', 'CA')\n",
        "temp9_df = job_db('ux+designer', 'Denver', 'CO')\n",
        "temp10_df = job_db('ux+designer', 'Huntsville', 'AL')\n",
        "temp11_df = job_db('ux+designer', 'Colorado+Springs', 'CO')\n",
        "temp12_df = job_db('ux+designer', 'Houston', 'TX')\n",
        "temp13_df = job_db('ux+designer', 'Trenton', 'NJ')\n",
        "temp14_df = job_db('ux+designer', 'Dallas', 'TX')\n",
        "temp15_df = job_db('ux+designer', 'Columbus', 'OH')\n",
        "temp16_df = job_db('ux+designer', 'Austin', 'TX')\n",
        "temp17_df = job_db('ux+designer', 'Philadelphia', 'PA')\n",
        "temp18_df = job_db('ux+designer', 'Durham', 'NC')\n",
        "temp19_df = job_db('ux+designer', 'Raleigh', 'NC')\n",
        "temp20_df = job_db('ux+designer', 'Atlanta', 'GA')\n",
        "\n",
        "ux_designer_df = pd.concat([temp1_df, temp2_df, temp3_df, temp4_df, temp5_df,\n",
        "                            temp6_df, temp7_df, temp8_df, temp9_df, temp10_df,\n",
        "                            temp11_df, temp12_df, temp13_df, temp14_df, temp15_df,\n",
        "                            temp16_df, temp17_df, temp18_df, temp19_df, temp20_df], ignore_index=True)\n",
        "print(ux_designer_df.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIxfH9yfnheV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp1_df = job_db('ios+developer', 'San+Jose', 'CA')\n",
        "temp2_df = job_db('ios+developer', 'San+Francisco', 'CA')\n",
        "temp3_df = job_db('ios+developer', 'Seattle', 'WA')\n",
        "temp4_df = job_db('ios+developer', 'Washington', 'DC')\n",
        "temp5_df = job_db('ios+developer', 'New+York', 'NY')\n",
        "temp6_df = job_db('ios+developer', 'Baltimore', 'MD')\n",
        "temp7_df = job_db('ios+developer', 'Boulder', 'CO')\n",
        "temp8_df = job_db('ios+developer', 'San+Diego', 'CA')\n",
        "temp9_df = job_db('ios+developer', 'Denver', 'CO')\n",
        "temp10_df = job_db('ios+developer', 'Huntsville', 'AL')\n",
        "temp11_df = job_db('ios+developer', 'Colorado+Springs', 'CO')\n",
        "temp12_df = job_db('ios+developer', 'Houston', 'TX')\n",
        "temp13_df = job_db('ios+developer', 'Trenton', 'NJ')\n",
        "temp14_df = job_db('ios+developer', 'Dallas', 'TX')\n",
        "temp15_df = job_db('ios+developer', 'Columbus', 'OH')\n",
        "temp16_df = job_db('ios+developer', 'Austin', 'TX')\n",
        "temp17_df = job_db('ios+developer', 'Philadelphia', 'PA')\n",
        "temp18_df = job_db('ios+developer', 'Durham', 'NC')\n",
        "temp19_df = job_db('ios+developer', 'Raleigh', 'NC')\n",
        "temp20_df = job_db('ios+developer', 'Atlanta', 'GA')\n",
        "\n",
        "ios_developer_df = pd.concat([temp1_df, temp2_df, temp3_df, temp4_df, temp5_df,\n",
        "                              temp6_df, temp7_df, temp8_df, temp9_df, temp10_df,\n",
        "                              temp11_df, temp12_df, temp13_df, temp14_df, temp15_df,\n",
        "                              temp16_df, temp17_df, temp18_df, temp19_df, temp20_df], ignore_index=True)\n",
        "print(ios_developer_df.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxO8nLwjrDOR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_scientist_df['job'] = 'data scientist'\n",
        "web_developer_df['job'] = 'web developer'\n",
        "ux_designer_df['job'] = 'ux designer'\n",
        "ios_developer_df['job'] = 'ios developer'\n",
        "indeed_df = pd.concat([data_scientist_df, web_developer_df, ux_designer_df, ios_developer_df], ignore_index=True)\n",
        "indeed_df = indeed_df.drop_duplicates(keep=False) \n",
        "indeed_df = indeed_df.reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQtq-mjCrDOW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_salary(sal_str, s_flag):\n",
        "    \n",
        "    sal_split = re.findall(r'\\d+', sal_str.replace(\",\", \"\"))\n",
        "    \n",
        "    if len(sal_split) == 2:\n",
        "        low_salary = int(sal_split[0])\n",
        "        high_salary = int(sal_split[1])\n",
        "    else:\n",
        "        low_salary = None\n",
        "        high_salary = None\n",
        "    \n",
        "    if s_flag == 'l':\n",
        "        salary = low_salary\n",
        "    else:\n",
        "        salary = high_salary\n",
        "    \n",
        "    return salary\n",
        "\n",
        "indeed_df['low_salary'] = indeed_df.apply(lambda x: convert_salary(x['salary'], 'l'), axis=1)\n",
        "indeed_df['high_salary'] = indeed_df.apply(lambda x: convert_salary(x['salary'], 'h'), axis=1)\n",
        "indeed_df = indeed_df.drop(columns=['salary'])\n",
        "\n",
        "indeed_df.head()                                      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQ8FO9aaixTW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "indeed_df.tail()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejMqC3iJrDOt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert to sqlite3 and csv\n",
        "conn = sqlite3.connect('techsearch.sqlite3')\n",
        "curs = conn.cursor()\n",
        "curs.execute('drop table if exists listings')\n",
        "indeed_df.to_sql('listings', con=conn)\n",
        "indeed_df.to_csv('/content/techsearch.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}